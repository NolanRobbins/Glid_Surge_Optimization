#!/usr/bin/env python3
"""
PRODUCTION GNN Training for Glid Surge Optimization
====================================================
Comprehensive Graph Neural Network training with:
- REAL surge labels from port activity data
- ALL 20+ features from every data source
- Route recommendation metrics (Precision@K, Hit Rate, Mode Accuracy)
- Full GPU utilization for competition demonstration

ASUS ASCENT GX10 (NVIDIA GB10 GRACE BLACKWELL):
- 1 petaFLOP AI performance
- 128GB unified memory
- Full graph training with comprehensive features

METRICS FOR ROUTE RECOMMENDATION:
- Surge Prediction: MAE, RMSE, R¬≤
- Mode Selection: Precision@K, Recall@K, Hit Rate
- Route Quality: Mode Accuracy (Rail vs Road decision)

Usage:
    python train_gnn.py --config production  # Full training with real data
    python train_gnn.py --config demo        # GPU stress test
    python train_gnn.py --config fast        # Quick test
"""

import os
import sys
import json
import time
import gc
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field

import numpy as np
import pandas as pd
import networkx as nx
from tqdm import tqdm
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

warnings.filterwarnings('ignore')
sys.path.insert(0, str(Path(__file__).parent / "src"))

# ============================================================================
# GPU SETUP AND MONITORING
# ============================================================================
print("="*80)
print("  GLID SURGE OPTIMIZATION - PRODUCTION GNN TRAINING")
print("  SAGEConv Graph Neural Network | Route Recommendation Engine")
print("="*80)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.backends.cudnn as cudnn

class GPUMonitor:
    """Track GPU utilization for competition demonstration."""
    
    def __init__(self):
        self.start_time = None
        self.peak_memory_gb = 0
        self.events = []
        
    def start(self):
        self.start_time = time.time()
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
            
    def log_event(self, name: str, details: str = ""):
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            mem_allocated = torch.cuda.memory_allocated() / 1e9
            mem_reserved = torch.cuda.memory_reserved() / 1e9
            peak = torch.cuda.max_memory_allocated() / 1e9
            self.peak_memory_gb = max(self.peak_memory_gb, peak)
            
            self.events.append({
                'name': name,
                'details': details,
                'time': time.time() - self.start_time,
                'allocated_gb': mem_allocated,
                'reserved_gb': mem_reserved,
                'peak_gb': peak
            })
            
    def get_summary(self) -> Dict:
        if not torch.cuda.is_available():
            return {}
        total_time = time.time() - self.start_time if self.start_time else 0
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        return {
            'total_time_seconds': total_time,
            'peak_gpu_memory_gb': self.peak_memory_gb,
            'gpu_name': torch.cuda.get_device_name(0),
            'gpu_total_memory_gb': total_mem,
            'gpu_utilization_pct': (self.peak_memory_gb / total_mem) * 100,
            'events': self.events
        }
    
    def print_summary(self):
        s = self.get_summary()
        if not s:
            return
        print("\n" + "="*80)
        print("  üñ•Ô∏è  GPU UTILIZATION REPORT")
        print("="*80)
        print(f"  ‚îå{'‚îÄ'*76}‚îê")
        print(f"  ‚îÇ  GPU: {s['gpu_name']:<68} ‚îÇ")
        print(f"  ‚îÇ  Total Memory: {s['gpu_total_memory_gb']:.1f} GB                                              ‚îÇ")
        print(f"  ‚îÇ  Peak Used: {s['peak_gpu_memory_gb']:.2f} GB ({s['gpu_utilization_pct']:.1f}% utilization)                          ‚îÇ")
        print(f"  ‚îÇ  Total Time: {s['total_time_seconds']:.1f}s                                                   ‚îÇ")
        print(f"  ‚îî{'‚îÄ'*76}‚îò")

gpu_monitor = GPUMonitor()

# GPU Setup
if torch.cuda.is_available():
    cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    DEVICE = torch.device('cuda')
    print(f"‚úì PyTorch: {torch.__version__}")
    print(f"‚úì GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB)")
    print(f"‚úì CUDA: {torch.version.cuda} | TF32: Enabled")
else:
    DEVICE = torch.device('cpu')
    print("‚ö† Running on CPU")

# PyTorch Geometric
try:
    import torch_geometric
    from torch_geometric.nn import SAGEConv, BatchNorm, GATConv
    from torch_geometric.data import Data
    from torch_geometric.loader import NeighborLoader
    HAS_PYG = True
    print(f"‚úì PyTorch Geometric: {torch_geometric.__version__}")
except ImportError:
    HAS_PYG = False
    print("‚úó PyTorch Geometric required"); sys.exit(1)

# RAPIDS cuGraph
try:
    import cugraph
    import cudf
    HAS_CUGRAPH = True
    print(f"‚úì NVIDIA cuGraph: GPU graph analytics")
except ImportError:
    HAS_CUGRAPH = False
    print("‚ö† cuGraph not available (CPU fallback)")

print("="*80)


# ============================================================================
# ROUTE RECOMMENDATION METRICS
# ============================================================================

class RouteRecommendationMetrics:
    """
    Metrics for evaluating route recommendation quality.
    
    The GNN predicts surge levels ‚Üí we use surge to recommend modes:
    - High surge (>0.5) ‚Üí Recommend RAIL (avoid congestion)
    - Low surge (‚â§0.5) ‚Üí ROAD is acceptable
    
    Metrics:
    - Precision@K: Of top-K surge predictions, how many were correct?
    - Hit Rate: Did we correctly identify high-surge periods?
    - Mode Accuracy: Rail vs Road recommendation accuracy
    """
    
    def __init__(self, surge_threshold: float = 0.5):
        self.surge_threshold = surge_threshold
        
    def compute_all(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        k_values: List[int] = [5, 10, 20]
    ) -> Dict[str, float]:
        """Compute all recommendation metrics."""
        
        metrics = {}
        
        # 1. Regression metrics
        metrics['mae'] = float(np.mean(np.abs(y_true - y_pred)))
        metrics['rmse'] = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - y_true.mean()) ** 2)
        metrics['r2'] = float(1 - ss_res / ss_tot) if ss_tot > 0 else 0.0
        
        # 2. Mode Classification (High surge ‚Üí Rail, Low surge ‚Üí Road)
        true_rail = (y_true > self.surge_threshold).astype(int)
        pred_rail = (y_pred > self.surge_threshold).astype(int)
        
        metrics['mode_accuracy'] = float(np.mean(true_rail == pred_rail))
        
        if true_rail.sum() > 0:
            metrics['rail_precision'] = float(precision_score(true_rail, pred_rail, zero_division=0))
            metrics['rail_recall'] = float(recall_score(true_rail, pred_rail, zero_division=0))
            metrics['rail_f1'] = float(f1_score(true_rail, pred_rail, zero_division=0))
        else:
            metrics['rail_precision'] = 0.0
            metrics['rail_recall'] = 0.0
            metrics['rail_f1'] = 0.0
        
        # 3. Precision@K (ranking quality)
        n = len(y_true)
        for k in k_values:
            if k > n:
                k = n
            # Top-K predicted surge indices
            top_k_pred_idx = np.argsort(y_pred)[-k:]
            # Top-K actual surge indices
            top_k_true_idx = set(np.argsort(y_true)[-k:])
            # How many of our top-K predictions are in actual top-K?
            hits = sum(1 for idx in top_k_pred_idx if idx in top_k_true_idx)
            metrics[f'precision_at_{k}'] = float(hits / k)
        
        # 4. Hit Rate (did we catch high-surge events?)
        high_surge_idx = set(np.where(y_true > self.surge_threshold)[0])
        if len(high_surge_idx) > 0:
            pred_high_idx = set(np.where(y_pred > self.surge_threshold)[0])
            hits = len(high_surge_idx & pred_high_idx)
            metrics['hit_rate'] = float(hits / len(high_surge_idx))
        else:
            metrics['hit_rate'] = 1.0  # No high-surge to catch
        
        # 5. NDCG (Normalized Discounted Cumulative Gain)
        metrics['ndcg'] = self._compute_ndcg(y_true, y_pred, k=min(20, n))
        
        return metrics
    
    def _compute_ndcg(self, y_true: np.ndarray, y_pred: np.ndarray, k: int) -> float:
        """Compute NDCG for surge ranking."""
        # Get ranking by predicted scores
        pred_order = np.argsort(y_pred)[::-1][:k]
        # Relevance scores from true values
        relevance = y_true[pred_order]
        # DCG
        dcg = np.sum(relevance / np.log2(np.arange(2, k + 2)))
        # Ideal DCG
        ideal_order = np.argsort(y_true)[::-1][:k]
        ideal_relevance = y_true[ideal_order]
        idcg = np.sum(ideal_relevance / np.log2(np.arange(2, k + 2)))
        return float(dcg / idcg) if idcg > 0 else 0.0
    
    def print_metrics(self, metrics: Dict, horizon: int):
        """Pretty print metrics."""
        print(f"\n  ‚îå{'‚îÄ'*74}‚îê")
        print(f"  ‚îÇ  {horizon}h SURGE PREDICTION & ROUTE RECOMMENDATION METRICS              ‚îÇ")
        print(f"  ‚îú{'‚îÄ'*74}‚î§")
        print(f"  ‚îÇ  REGRESSION:                                                            ‚îÇ")
        print(f"  ‚îÇ    MAE: {metrics['mae']:.4f}  RMSE: {metrics['rmse']:.4f}  R¬≤: {metrics['r2']:.4f}                         ‚îÇ")
        print(f"  ‚îú{'‚îÄ'*74}‚î§")
        print(f"  ‚îÇ  ROUTE RECOMMENDATION (Rail vs Road):                                   ‚îÇ")
        print(f"  ‚îÇ    Mode Accuracy: {metrics['mode_accuracy']*100:.1f}%                                              ‚îÇ")
        print(f"  ‚îÇ    Rail Precision: {metrics['rail_precision']*100:.1f}%  Recall: {metrics['rail_recall']*100:.1f}%  F1: {metrics['rail_f1']:.3f}            ‚îÇ")
        print(f"  ‚îú{'‚îÄ'*74}‚î§")
        print(f"  ‚îÇ  RANKING QUALITY:                                                       ‚îÇ")
        print(f"  ‚îÇ    Precision@5: {metrics.get('precision_at_5', 0)*100:.1f}%  @10: {metrics.get('precision_at_10', 0)*100:.1f}%  @20: {metrics.get('precision_at_20', 0)*100:.1f}%             ‚îÇ")
        print(f"  ‚îÇ    Hit Rate: {metrics['hit_rate']*100:.1f}%  NDCG: {metrics['ndcg']:.4f}                                    ‚îÇ")
        print(f"  ‚îî{'‚îÄ'*74}‚îò")


# ============================================================================
# GNN MODEL
# ============================================================================

class SurgeGNN(nn.Module):
    """SAGEConv GNN for surge prediction with route recommendation."""
    
    def __init__(self, in_channels: int, hidden_channels: int = 256, 
                 out_channels: int = 1, num_layers: int = 3, dropout: float = 0.2):
        super().__init__()
        self.num_layers = num_layers
        self.dropout = dropout
        
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        
        self.convs.append(SAGEConv(in_channels, hidden_channels))
        self.norms.append(BatchNorm(hidden_channels))
        
        for _ in range(num_layers - 1):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels))
            self.norms.append(BatchNorm(hidden_channels))
        
        self.predictor = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Linear(hidden_channels // 2, out_channels),
            nn.Sigmoid()
        )
        
        self.n_params = sum(p.numel() for p in self.parameters())
    
    def forward(self, x, edge_index):
        for conv, norm in zip(self.convs, self.norms):
            x = conv(x, edge_index)
            x = norm(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        return self.predictor(x)


# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass  
class GNNConfig:
    """Training configuration."""
    
    # Model
    hidden_channels: int = 256
    num_layers: int = 3
    dropout: float = 0.2
    
    # Training
    learning_rate: float = 0.001
    weight_decay: float = 1e-4
    epochs: int = 200
    patience: int = 30
    use_amp: bool = True
    
    # Data loading
    ais_sample_n: Optional[int] = None  # None = full data
    truck_sample_frac: float = 1.0      # 1.0 = full data
    
    # Features
    use_real_labels: bool = True
    surge_threshold: float = 0.5
    
    @classmethod
    def production(cls):
        """Full production training with all data."""
        return cls(
            hidden_channels=256,
            num_layers=3,
            epochs=200,
            patience=30,
            ais_sample_n=None,  # Full 823K
            truck_sample_frac=1.0,
            use_real_labels=True
        )
    
    @classmethod
    def demo(cls):
        """Moderate GPU usage for quick demos."""
        return cls(
            hidden_channels=512,
            num_layers=4,
            epochs=100,
            patience=20,
            ais_sample_n=200000,
            truck_sample_frac=0.2,
            use_real_labels=True
        )
    
    @classmethod
    def stress_test(cls):
        """
        MAXIMUM GPU STRESS TEST - Push the GX10 to its limits!
        
        This config demonstrates the full power of the ASUS Ascent GX10:
        - 1024 hidden channels (4x production)
        - 6 message passing layers (deep network)
        - Full graph training on ALL 197K nodes
        - Synthetic labels to enable full-graph supervision
        - ~8M+ parameters
        
        Expected GPU usage:
        - Memory: 20-40 GB (15-30% of 128GB)
        - Training time: 30-60 minutes
        - Throughput: Processing entire US rail network
        
        SPARK STORY: "We loaded the ENTIRE US rail network (197K nodes, 
        225K edges) into GPU memory and trained a 6-layer GNN with message 
        passing that would take HOURS on CPU. The 128GB unified memory 
        allows us to hold the full graph + model + gradients simultaneously."
        """
        return cls(
            hidden_channels=1024,   # MASSIVE hidden dimensions
            num_layers=6,           # DEEP message passing
            dropout=0.3,
            learning_rate=0.0005,   # Lower LR for stability
            epochs=150,
            patience=30,
            ais_sample_n=None,      # FULL AIS data
            truck_sample_frac=1.0,  # FULL truck data
            use_real_labels=False,  # Synthetic for full-graph training
        )
    
    @classmethod
    def fast(cls):
        """Quick test."""
        return cls(
            hidden_channels=64,
            num_layers=2,
            epochs=30,
            patience=10,
            ais_sample_n=50000,
            truck_sample_frac=0.05,
            use_real_labels=True
        )


# ============================================================================
# DATA LOADING - FULL DATA
# ============================================================================

def load_all_data(config: GNNConfig) -> Dict[str, Any]:
    """Load ALL data sources for production training."""
    from data.loaders import (
        load_rail_nodes, load_rail_lines, load_port_activity,
        load_weather_data, load_ais_vessels, load_truck_times,
        load_portwatch_chokepoints
    )
    from config import LOGISTICS_FLEET_DIR
    
    print("\n" + "="*80)
    print("  LOADING ALL DATA SOURCES (Production Mode)")
    print("="*80)
    
    data = {}
    
    # 1. Rail Network (FULL)
    print("\n[1/8] Rail Network (USDOT NTAD)...")
    data['rail_nodes'] = load_rail_nodes(filter_us_only=True)
    data['rail_lines'] = load_rail_lines(filter_us_only=True)
    print(f"  ‚úì {len(data['rail_nodes']):,} nodes, {len(data['rail_lines']):,} edges")
    
    # 2. Port Activity (FULL - for REAL labels)
    print("\n[2/8] Global Daily Port Activity (IMF PortWatch)...")
    major_ports = [
        "Los Angeles-Long Beach", "New York-New Jersey", "Savannah",
        "Houston", "Oakland", "Seattle", "Tacoma", "Virginia",
        "Charleston", "Miami", "New Orleans", "Baltimore"
    ]
    data['port_activity'] = load_port_activity(ports=major_ports, country="United States")
    n_ports = data['port_activity']['portname'].nunique()
    n_days = data['port_activity']['date'].nunique()
    print(f"  ‚úì {len(data['port_activity']):,} records ({n_ports} ports √ó {n_days} days)")
    
    # 3. Weather (FULL)
    print("\n[3/8] Weather Data (Open-Meteo)...")
    try:
        data['weather'] = load_weather_data(hourly=False)
        data['weather']['date'] = pd.to_datetime(data['weather']['date']).dt.tz_localize(None)
        print(f"  ‚úì {len(data['weather']):,} daily records")
    except Exception as e:
        print(f"  ‚ö† Weather not available: {e}")
        data['weather'] = None
    
    # 4. AIS Vessels (configurable)
    print("\n[4/8] AIS Vessel Tracking...")
    try:
        if config.ais_sample_n:
            data['ais'] = load_ais_vessels(sample_n=config.ais_sample_n)
            print(f"  ‚úì {len(data['ais']):,} records (sampled)")
        else:
            data['ais'] = load_ais_vessels(sample_n=None)
            print(f"  ‚úì {len(data['ais']):,} records (FULL)")
    except Exception as e:
        print(f"  ‚ö† AIS not available: {e}")
        data['ais'] = None
    
    # 5. Truck Travel Times (configurable - for edge weights)
    print("\n[5/8] Truck Travel Times (BTS/ATRI)...")
    try:
        data['truck_times'] = load_truck_times(sample_frac=config.truck_sample_frac)
        print(f"  ‚úì {len(data['truck_times']):,} county-to-county routes")
    except Exception as e:
        print(f"  ‚ö† Truck times not available: {e}")
        data['truck_times'] = None
    
    # 6. PortWatch Chokepoints
    print("\n[6/8] PortWatch Chokepoints...")
    try:
        data['chokepoints'] = load_portwatch_chokepoints()
        print(f"  ‚úì {len(data['chokepoints']):,} chokepoint transit records")
    except Exception as e:
        print(f"  ‚ö† Chokepoints not available: {e}")
        data['chokepoints'] = None
    
    # 7. Logistics Fleet
    print("\n[7/8] Logistics Fleet Data...")
    try:
        freight_path = LOGISTICS_FLEET_DIR / "fFreight.csv"
        if freight_path.exists():
            data['freight_rates'] = pd.read_csv(freight_path)
            print(f"  ‚úì Freight rates: {len(data['freight_rates']):,} records")
        else:
            data['freight_rates'] = None
            
        costs_path = LOGISTICS_FLEET_DIR / "fCosts.xlsx"
        if costs_path.exists():
            data['fleet_costs'] = pd.read_excel(costs_path)
            print(f"  ‚úì Fleet costs: {len(data['fleet_costs']):,} records")
        else:
            data['fleet_costs'] = None
    except Exception as e:
        print(f"  ‚ö† Fleet data error: {e}")
        data['freight_rates'] = None
        data['fleet_costs'] = None
    
    print("\n[8/8] Data Loading Complete!")
    
    # Summary
    loaded = sum(1 for k, v in data.items() if v is not None)
    print(f"\n  ‚úì Loaded {loaded} data sources successfully")
    
    return data


# ============================================================================
# COMPREHENSIVE FEATURE ENGINEERING
# ============================================================================

def compute_real_surge_labels(
    port_activity_df: pd.DataFrame,
    horizon_hours: int = 24
) -> pd.DataFrame:
    """
    Compute REAL surge labels from port activity data.
    
    Surge = (future_activity - current_activity) / current_activity
    Normalized to [0, 1] range.
    """
    df = port_activity_df.copy()
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(['portname', 'date'])
    
    horizon_days = horizon_hours // 24
    
    # Compute future port calls
    df['future_portcalls'] = df.groupby('portname')['portcalls'].shift(-horizon_days)
    
    # Compute surge
    df['surge_raw'] = (df['future_portcalls'] - df['portcalls']) / df['portcalls'].replace(0, 1)
    
    # Normalize to [0, 1]
    surge_min = df['surge_raw'].quantile(0.01)
    surge_max = df['surge_raw'].quantile(0.99)
    df[f'surge_{horizon_hours}h'] = np.clip(
        (df['surge_raw'] - surge_min) / (surge_max - surge_min + 1e-8),
        0, 1
    )
    
    # Drop rows without future data
    df = df.dropna(subset=[f'surge_{horizon_hours}h'])
    
    return df


def build_comprehensive_features(
    port_activity_df: pd.DataFrame,
    weather_df: pd.DataFrame = None,
    ais_df: pd.DataFrame = None,
    chokepoints_df: pd.DataFrame = None,
    truck_times_df: pd.DataFrame = None,
    horizon_hours: int = 24
) -> Tuple[pd.DataFrame, List[str]]:
    """
    Build comprehensive feature set (20+ features) from all data sources.
    
    Features:
    - Time: day_sin, day_cos, month_sin, month_cos, is_weekend (5)
    - Port activity lags: 1d, 2d, 3d, 7d, 14d (5)
    - Rolling stats: 3d_mean, 7d_mean, 14d_mean, 7d_std (4)
    - Weather: precip, wind, weather_impact (3)
    - AIS vessels: inbound_count (1)
    - Chokepoints: global_pressure (1)
    - Truck context: avg_travel_time (1)
    - Total: 20+ features
    """
    print(f"\n  Building features for {horizon_hours}h horizon...")
    
    df = port_activity_df.copy()
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values(['portname', 'date'])
    
    feature_cols = []
    
    # 1. TIME FEATURES (5)
    df['day_of_year'] = df['date'].dt.dayofyear
    df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
    df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)
    df['month'] = df['date'].dt.month
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    df['dayofweek'] = df['date'].dt.dayofweek
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(float)
    feature_cols.extend(['day_sin', 'day_cos', 'month_sin', 'month_cos', 'is_weekend'])
    print(f"    ‚úì Time features: 5")
    
    # 2. LAG FEATURES (5)
    for lag in [1, 2, 3, 7, 14]:
        col = f'portcalls_lag_{lag}d'
        df[col] = df.groupby('portname')['portcalls'].shift(lag)
        feature_cols.append(col)
    print(f"    ‚úì Lag features: 5")
    
    # 3. ROLLING FEATURES (4)
    for window in [3, 7, 14]:
        col_mean = f'portcalls_roll_{window}d_mean'
        df[col_mean] = df.groupby('portname')['portcalls'].transform(
            lambda x: x.rolling(window, min_periods=1).mean()
        )
        feature_cols.append(col_mean)
    
    df['portcalls_roll_7d_std'] = df.groupby('portname')['portcalls'].transform(
        lambda x: x.rolling(7, min_periods=1).std()
    )
    feature_cols.append('portcalls_roll_7d_std')
    print(f"    ‚úì Rolling features: 4")
    
    # 4. WEATHER FEATURES (3)
    if weather_df is not None and len(weather_df) > 0:
        weather = weather_df.copy()
        # Handle timezone - ensure both are timezone-naive
        weather['date'] = pd.to_datetime(weather['date'])
        if weather['date'].dt.tz is not None:
            weather['date'] = weather['date'].dt.tz_localize(None)
        weather_agg = weather.groupby('date').agg({
            'precipitation_sum': 'mean',
            'wind_speed_10m_max': 'mean'
        }).reset_index()
        weather_agg.columns = ['date', 'precip_avg', 'wind_avg']
        # Ensure df date is also timezone-naive
        df['date'] = pd.to_datetime(df['date'])
        if df['date'].dt.tz is not None:
            df['date'] = df['date'].dt.tz_localize(None)
        df = df.merge(weather_agg, on='date', how='left')
        df['precip_avg'] = df['precip_avg'].fillna(0)
        df['wind_avg'] = df['wind_avg'].fillna(0)
        df['weather_impact'] = (
            np.clip(df['precip_avg'] / 20, 0, 1) * 0.6 +
            np.clip(df['wind_avg'] / 50, 0, 1) * 0.4
        )
        feature_cols.extend(['precip_avg', 'wind_avg', 'weather_impact'])
        print(f"    ‚úì Weather features: 3")
    else:
        df['precip_avg'] = 0.0
        df['wind_avg'] = 0.0
        df['weather_impact'] = 0.0
        feature_cols.extend(['precip_avg', 'wind_avg', 'weather_impact'])
    
    # 5. AIS VESSEL FEATURES (1)
    if ais_df is not None and len(ais_df) > 0:
        ais = ais_df.copy()
        # Get date from eta if available
        if 'eta' in ais.columns:
            ais['eta'] = pd.to_datetime(ais['eta'], errors='coerce')
            ais['eta_date'] = ais['eta'].dt.date
            ais_daily = ais.groupby('eta_date').size().reset_index(name='vessel_inbound')
            ais_daily['date'] = pd.to_datetime(ais_daily['eta_date'])
            df = df.merge(ais_daily[['date', 'vessel_inbound']], on='date', how='left')
            df['vessel_inbound'] = df['vessel_inbound'].fillna(0)
            # Normalize
            df['vessel_inbound_norm'] = df['vessel_inbound'] / (df['vessel_inbound'].max() + 1)
        else:
            df['vessel_inbound_norm'] = 0.0
        feature_cols.append('vessel_inbound_norm')
        print(f"    ‚úì AIS vessel features: 1")
    else:
        df['vessel_inbound_norm'] = 0.0
        feature_cols.append('vessel_inbound_norm')
    
    # 6. CHOKEPOINT FEATURES (1)
    if chokepoints_df is not None and len(chokepoints_df) > 0:
        choke = chokepoints_df.copy()
        if 'date' in choke.columns and 'transitCalls' in choke.columns:
            choke['date'] = pd.to_datetime(choke['date']).dt.tz_localize(None)
            choke_daily = choke.groupby('date')['transitCalls'].sum().reset_index()
            choke_daily.columns = ['date', 'chokepoint_calls']
            df = df.merge(choke_daily, on='date', how='left')
            df['chokepoint_calls'] = df['chokepoint_calls'].fillna(0)
            df['chokepoint_pressure'] = np.clip(df['chokepoint_calls'] / 1000, 0, 1)
        else:
            df['chokepoint_pressure'] = 0.0
        feature_cols.append('chokepoint_pressure')
        print(f"    ‚úì Chokepoint features: 1")
    else:
        df['chokepoint_pressure'] = 0.0
        feature_cols.append('chokepoint_pressure')
    
    # 7. TRUCK CONTEXT (1)
    if truck_times_df is not None and len(truck_times_df) > 0:
        time_col = [c for c in truck_times_df.columns if 'time' in c.lower() or 'minute' in c.lower()]
        if time_col:
            avg_time = truck_times_df[time_col[0]].mean()
            df['avg_drayage_time'] = avg_time / 100  # Normalized
        else:
            df['avg_drayage_time'] = 0.0
        feature_cols.append('avg_drayage_time')
        print(f"    ‚úì Truck context features: 1")
    else:
        df['avg_drayage_time'] = 0.0
        feature_cols.append('avg_drayage_time')
    
    # Clean up
    df = df.dropna(subset=feature_cols)
    
    print(f"    ‚úì Total features: {len(feature_cols)}")
    print(f"    ‚úì Samples: {len(df):,}")
    
    return df, feature_cols


# ============================================================================
# GRAPH CONSTRUCTION
# ============================================================================

def build_transport_graph(
    rail_nodes_gdf, 
    rail_lines_gdf,
    truck_times_df: pd.DataFrame = None
) -> nx.Graph:
    """Build transportation graph with optional truck time edge weights."""
    from graph.builder import build_rail_graph, add_location_nodes, connect_locations_to_graph
    from config import US_PORTS, RAIL_TERMINALS
    
    print("\n" + "‚îÄ"*80)
    print("  Building Transportation Graph")
    print("‚îÄ"*80)
    
    G = build_rail_graph(rail_nodes_gdf, rail_lines_gdf)
    print(f"  ‚úì Rail network: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges")
    
    # Add ports
    ports_dict = {p["name"]: p for p in US_PORTS}
    G = add_location_nodes(G, ports_dict, "port")
    G = connect_locations_to_graph(G, ports_dict, "port", max_connection_miles=30)
    
    # Add terminals
    terminals_dict = {t["name"]: t for t in RAIL_TERMINALS}
    G = add_location_nodes(G, terminals_dict, "terminal")
    G = connect_locations_to_graph(G, terminals_dict, "terminal", max_connection_miles=30)
    
    # Add truck time weights if available
    if truck_times_df is not None and len(truck_times_df) > 0:
        time_col = [c for c in truck_times_df.columns if 'time' in c.lower() or 'minute' in c.lower()]
        if time_col:
            avg_truck_time = truck_times_df[time_col[0]].mean()
            # Apply to road connections (ports/terminals)
            for u, v in G.edges():
                if G.nodes[u].get('type') in ['port', 'terminal'] or G.nodes[v].get('type') in ['port', 'terminal']:
                    G[u][v]['truck_time'] = avg_truck_time
            print(f"  ‚úì Added truck time weights (avg: {avg_truck_time:.1f} min)")
    
    print(f"  ‚úì Final graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges")
    return G


def compute_graph_features(G: nx.Graph) -> Tuple[Dict, List, Dict]:
    """Compute graph centrality features using cuGraph if available."""
    print("\n" + "‚îÄ"*80)
    print("  Computing Graph Centrality Features")
    print("‚îÄ"*80)
    
    gpu_monitor.log_event("graph_features_start")
    
    node_list = list(G.nodes())
    node_to_idx = {n: i for i, n in enumerate(node_list)}
    
    if HAS_CUGRAPH:
        print("  Using NVIDIA cuGraph (GPU-accelerated)...")
        
        edges = list(G.edges())
        source = [node_to_idx[u] for u, v in edges]
        destination = [node_to_idx[v] for u, v in edges]
        
        gdf = cudf.DataFrame({'source': source, 'destination': destination})
        cu_G = cugraph.Graph()
        cu_G.from_cudf_edgelist(gdf, source='source', destination='destination')
        
        # PageRank
        t0 = time.time()
        pr_df = cugraph.pagerank(cu_G)
        pr_dict = dict(zip(pr_df['vertex'].to_pandas(), pr_df['pagerank'].to_pandas()))
        print(f"    ‚úì PageRank: {time.time()-t0:.2f}s")
        
        # Degree
        deg_df = cu_G.degrees()
        if 'in_degree' in deg_df.columns:
            deg_series = deg_df['in_degree'] + deg_df['out_degree']
        else:
            deg_series = deg_df['degree']
        deg_dict = dict(zip(deg_df['vertex'].to_pandas(), deg_series.to_pandas()))
        
        # Betweenness (heavy computation)
        t0 = time.time()
        try:
            bc_df = cugraph.betweenness_centrality(cu_G, k=500)  # More samples for accuracy
            bc_dict = dict(zip(bc_df['vertex'].to_pandas(), bc_df['betweenness_centrality'].to_pandas()))
            print(f"    ‚úì Betweenness (k=500): {time.time()-t0:.2f}s")
        except Exception as e:
            print(f"    ‚ö† Betweenness failed: {e}")
            bc_dict = {i: 0.0 for i in range(len(node_list))}
    else:
        print("  Using NetworkX (CPU)...")
        pagerank_raw = nx.pagerank(G, max_iter=100)
        pr_dict = {node_to_idx[n]: v for n, v in pagerank_raw.items()}
        deg_dict = {node_to_idx[n]: G.degree(n) for n in node_list}
        bc_dict = {i: 0.0 for i in range(len(node_list))}
    
    gpu_monitor.log_event("graph_features_end")
    
    # Build feature dict
    # Note: graph builder uses 'node_type' attribute
    features = {}
    for node in node_list:
        idx = node_to_idx[node]
        node_data = G.nodes[node]
        node_type = node_data.get('node_type', 'rail_node')
        
        features[node] = np.array([
            pr_dict.get(idx, 0.0) * 10000,  # PageRank scaled
            deg_dict.get(idx, 0) / 10,       # Degree normalized
            bc_dict.get(idx, 0.0) * 1000,    # Betweenness scaled
            1.0 if node_type == 'port' else 0.0,
            1.0 if node_type == 'terminal' else 0.0,
        ], dtype=np.float32)
    
    print(f"  ‚úì Graph features: {len(features):,} nodes √ó 5 features")
    
    return features, node_list, node_to_idx


# ============================================================================
# TRAINING
# ============================================================================

def train_gnn(
    G: nx.Graph,
    graph_features: Dict[str, np.ndarray],
    port_features_df: pd.DataFrame,
    feature_cols: List[str],
    node_list: List,
    node_to_idx: Dict,
    config: GNNConfig,
    horizon: int = 24
) -> Tuple[SurgeGNN, Dict]:
    """Train GNN with real surge labels and comprehensive features."""
    
    print("\n" + "="*80)
    print(f"  TRAINING GNN ({horizon}h horizon)")
    print("="*80)
    
    gpu_monitor.log_event("training_start", f"{horizon}h")
    
    # Mapping from port activity data names to graph node names
    PORT_DATA_TO_GRAPH = {
        "Los Angeles-Long Beach": "Port of Los Angeles",
        "New York-New Jersey": "Port of New York/New Jersey",
        "Savannah": "Port of Savannah",
        "Houston": "Port of Houston",
        "Oakland": "Port of Oakland",
        "Seattle": "Port of Seattle",
        "Tacoma": "Port of Tacoma",
        "Virginia": "Port of Virginia (Norfolk)",
        "Charleston": "Port of Charleston",
        "Miami": "Port of Miami",
        "New Orleans": "Port of New Orleans",
        "Baltimore": "Port of Baltimore",
    }
    
    # Build mapping: data port name -> node index
    port_name_to_node = {}
    
    # First pass: find all port nodes and their graph names
    # Note: graph builder uses 'node_type' attribute (not 'type')
    graph_name_to_node = {}
    
    for n in node_list:
        node_data = G.nodes[n]
        node_type = node_data.get('node_type', None)
        if node_type == 'port':
            graph_name = node_data.get('name', str(n))
            graph_name_to_node[graph_name] = n
    
    print(f"  ‚úì Found {len(graph_name_to_node)} port nodes in graph")
    
    # Second pass: map data names to nodes
    for data_name, graph_name in PORT_DATA_TO_GRAPH.items():
        if graph_name in graph_name_to_node:
            port_name_to_node[data_name] = graph_name_to_node[graph_name]
    
    # Build node feature matrix
    num_nodes = len(node_list)
    graph_feat_dim = 5
    port_feat_dim = len(feature_cols)
    total_feat_dim = graph_feat_dim + port_feat_dim
    
    X = np.zeros((num_nodes, total_feat_dim), dtype=np.float32)
    y = np.full(num_nodes, 0.5, dtype=np.float32)  # Default
    labeled_mask = np.zeros(num_nodes, dtype=bool)
    
    # Fill graph features for all nodes
    for node in node_list:
        idx = node_to_idx[node]
        X[idx, :graph_feat_dim] = graph_features[node]
    
    # Fill port features and labels
    surge_col = f'surge_{horizon}h'
    
    # Match port data to graph nodes
    matched_ports = set()
    for _, row in port_features_df.iterrows():
        port_name = row['portname']
        if port_name in port_name_to_node:
            node = port_name_to_node[port_name]
            idx = node_to_idx[node]
            
            # Port features
            port_feats = row[feature_cols].values.astype(np.float32)
            X[idx, graph_feat_dim:] = port_feats
            
            # Surge label
            if surge_col in port_features_df.columns and not pd.isna(row[surge_col]):
                y[idx] = float(row[surge_col])
                labeled_mask[idx] = True
                matched_ports.add(port_name)
    
    print(f"  ‚úì Matched {len(matched_ports)} ports to graph nodes")
    
    n_labeled = labeled_mask.sum()
    print(f"  ‚úì Features: {total_feat_dim} dims ({graph_feat_dim} graph + {port_feat_dim} port)")
    print(f"  ‚úì Labeled nodes: {n_labeled} (ports with surge data)")
    
    if n_labeled < 5:
        print(f"  ‚ö† Not enough labeled data for {horizon}h horizon")
        return None, {}
    
    # Clean data
    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Build edge index
    edges = list(G.edges())
    edge_index = np.array([
        [node_to_idx[u] for u, v in edges] + [node_to_idx[v] for u, v in edges],
        [node_to_idx[v] for u, v in edges] + [node_to_idx[u] for u, v in edges]
    ], dtype=np.int64)
    
    # Transfer to GPU
    print(f"  Transferring to GPU...")
    data = Data(
        x=torch.FloatTensor(X),
        edge_index=torch.LongTensor(edge_index),
        y=torch.FloatTensor(y).unsqueeze(1)
    ).to(DEVICE)
    
    gpu_monitor.log_event("data_to_gpu", f"Graph: {num_nodes:,} nodes")
    
    # Create masks (split labeled nodes 80/20)
    labeled_idx = np.where(labeled_mask)[0]
    np.random.shuffle(labeled_idx)
    split = int(0.8 * len(labeled_idx))
    train_idx = labeled_idx[:split]
    val_idx = labeled_idx[split:]
    
    train_mask = torch.zeros(num_nodes, dtype=torch.bool, device=DEVICE)
    val_mask = torch.zeros(num_nodes, dtype=torch.bool, device=DEVICE)
    train_mask[train_idx] = True
    val_mask[val_idx] = True
    
    print(f"  ‚úì Train: {len(train_idx)}, Val: {len(val_idx)}")
    print(f"  ‚úì Graph: {num_nodes:,} nodes, {len(edges):,} edges")
    
    # Model
    model = SurgeGNN(
        in_channels=total_feat_dim,
        hidden_channels=config.hidden_channels,
        out_channels=1,
        num_layers=config.num_layers,
        dropout=config.dropout
    ).to(DEVICE)
    
    print(f"\n  Model: {model.n_params:,} parameters")
    
    # Training setup
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)
    criterion = nn.MSELoss()
    
    use_amp = config.use_amp and DEVICE.type == 'cuda'
    scaler = torch.amp.GradScaler('cuda') if use_amp else None
    
    best_val_loss = float('inf')
    best_state = None
    patience_counter = 0
    
    print(f"\n  Training for {config.epochs} epochs...")
    pbar = tqdm(range(config.epochs), desc="  Training", unit="epoch")
    
    for epoch in pbar:
        # Training
        model.train()
        optimizer.zero_grad()
        
        if use_amp:
            with torch.amp.autocast('cuda'):
                out = model(data.x, data.edge_index)
                loss = criterion(out[train_mask], data.y[train_mask])
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            out = model(data.x, data.edge_index)
            loss = criterion(out[train_mask], data.y[train_mask])
            loss.backward()
            optimizer.step()
        
        # Validation
        model.eval()
        with torch.no_grad():
            out = model(data.x, data.edge_index)
            val_loss = criterion(out[val_mask], data.y[val_mask])
        
        scheduler.step(val_loss)
        
        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'val': f'{val_loss.item():.4f}'})
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= config.patience:
                print(f"\n  Early stopping at epoch {epoch}")
                break
    
    gpu_monitor.log_event("training_end", f"{horizon}h")
    
    # Restore best model
    if best_state:
        model.load_state_dict(best_state)
    
    # Evaluate with route recommendation metrics
    model.eval()
    with torch.no_grad():
        out = model(data.x, data.edge_index)
        y_pred = out[val_mask].cpu().numpy().flatten()
        y_true = data.y[val_mask].cpu().numpy().flatten()
    
    rec_metrics = RouteRecommendationMetrics(surge_threshold=config.surge_threshold)
    metrics = rec_metrics.compute_all(y_true, y_pred)
    rec_metrics.print_metrics(metrics, horizon)
    
    # Inference benchmark
    torch.cuda.synchronize()
    t0 = time.time()
    for _ in range(10):
        with torch.no_grad():
            _ = model(data.x, data.edge_index)
    torch.cuda.synchronize()
    metrics['inference_time_ms'] = (time.time() - t0) / 10 * 1000
    metrics['throughput_nodes_per_sec'] = num_nodes / metrics['inference_time_ms'] * 1000
    
    print(f"\n  ‚ö° Inference: {metrics['inference_time_ms']:.1f}ms ({metrics['throughput_nodes_per_sec']:,.0f} nodes/sec)")
    
    return model, metrics


def save_model(model, metrics, config, horizon):
    """Save model checkpoint with full metadata."""
    checkpoint_dir = Path("output/checkpoints")
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = checkpoint_dir / f"gnn_production_{horizon}h_{timestamp}.pt"
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'metrics': metrics,
        'config': {
            'hidden_channels': config.hidden_channels,
            'num_layers': config.num_layers,
            'surge_threshold': config.surge_threshold,
        },
        'timestamp': timestamp,
    }, model_path)
    
    print(f"  ‚úì Saved: {model_path}")
    return model_path


# ============================================================================
# MAIN
# ============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Production GNN Training")
    parser.add_argument('--config', choices=['production', 'demo', 'fast', 'stress_test'], default='production')
    parser.add_argument('--horizons', type=str, default='24,48,72')
    parser.add_argument('--epochs', type=int, default=None)
    args = parser.parse_args()
    
    # Config
    if args.config == 'production':
        config = GNNConfig.production()
        print("\nüöÄ PRODUCTION MODE: Full data, real labels, comprehensive features")
    elif args.config == 'stress_test':
        config = GNNConfig.stress_test()
        print("\n" + "üî•"*40)
        print("  GPU STRESS TEST MODE - PUSHING THE GX10 TO ITS LIMITS!")
        print("  - 1024 hidden channels, 6 layers deep")
        print("  - Full graph training on 197K nodes")
        print("  - Expected: 20-40 GB GPU memory, 30-60 min training")
        print("üî•"*40)
    elif args.config == 'demo':
        config = GNNConfig.demo()
        print("\nüñ•Ô∏è  DEMO MODE: Moderate GPU usage")
    else:
        config = GNNConfig.fast()
        print("\n‚ö° FAST MODE: Quick test")
    
    if args.epochs:
        config.epochs = args.epochs
    
    horizons = [int(h.strip()) for h in args.horizons.split(',')]
    
    print(f"\n[Config] {args.config}")
    print(f"  Horizons: {horizons}")
    print(f"  Hidden: {config.hidden_channels}, Layers: {config.num_layers}")
    print(f"  Epochs: {config.epochs}, Patience: {config.patience}")
    print(f"  AIS: {'Full' if config.ais_sample_n is None else f'{config.ais_sample_n:,} sample'}")
    print(f"  Truck data: {config.truck_sample_frac*100:.0f}%")
    
    gpu_monitor.start()
    
    # Load data
    data = load_all_data(config)
    gpu_monitor.log_event("data_loaded")
    
    # Build graph
    G = build_transport_graph(
        data['rail_nodes'], 
        data['rail_lines'],
        data.get('truck_times')
    )
    gpu_monitor.log_event("graph_built")
    
    # Graph features
    graph_features, node_list, node_to_idx = compute_graph_features(G)
    
    # Train for each horizon
    results = {}
    
    for horizon in horizons:
        # Compute surge labels
        port_df = compute_real_surge_labels(data['port_activity'], horizon)
        
        # Build features
        port_features_df, feature_cols = build_comprehensive_features(
            port_df,
            weather_df=data.get('weather'),
            ais_df=data.get('ais'),
            chokepoints_df=data.get('chokepoints'),
            truck_times_df=data.get('truck_times'),
            horizon_hours=horizon
        )
        
        # Train
        model, metrics = train_gnn(
            G, graph_features, port_features_df, feature_cols,
            node_list, node_to_idx, config, horizon
        )
        
        if model is not None:
            save_model(model, metrics, config, horizon)
            results[horizon] = metrics
    
    # GPU Summary
    gpu_monitor.print_summary()
    
    # Final results
    print("\n" + "="*80)
    print("  ‚úÖ PRODUCTION GNN TRAINING COMPLETE")
    print("="*80)
    print("\n  SURGE PREDICTION METRICS:")
    for h, m in results.items():
        print(f"    {h}h: R¬≤={m['r2']:.4f}, MAE={m['mae']:.4f}, RMSE={m['rmse']:.4f}")
    
    print("\n  ROUTE RECOMMENDATION METRICS:")
    for h, m in results.items():
        print(f"    {h}h: ModeAcc={m['mode_accuracy']*100:.1f}%, HitRate={m['hit_rate']*100:.1f}%, NDCG={m['ndcg']:.4f}")
    
    print("\n  INFERENCE PERFORMANCE:")
    for h, m in results.items():
        print(f"    {h}h: {m['inference_time_ms']:.1f}ms ({m['throughput_nodes_per_sec']:,.0f} nodes/sec)")
    
    print("="*80)
    
    # Save comprehensive metadata
    gpu_summary = gpu_monitor.get_summary()
    metadata = {
        'config': args.config,
        'horizons': horizons,
        'results': {str(k): v for k, v in results.items()},
        'gpu_stats': gpu_summary,
        'timestamp': datetime.now().isoformat()
    }
    
    with open('output/checkpoints/production_training_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2, default=str)
    
    print(f"\n  üìä Metadata saved to output/checkpoints/production_training_metadata.json")
    
    return results


if __name__ == "__main__":
    main()
