#!/usr/bin/env python3
"""
GPU-Accelerated Training Script for Glid Surge Optimization
============================================================
Optimized for ASUS Ascent GX10 with NVIDIA GB10 Grace Blackwell Superchip
- 1 petaFLOP AI performance
- 128GB unified memory
- NVIDIA Blackwell architecture

Usage:
    python train_gpu.py                    # Default training
    python train_gpu.py --config accurate  # Maximum accuracy
    python train_gpu.py --config fast      # Quick prototyping
"""

import os
import sys
import json
import pickle
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional, Dict, Any

# ============================================================================
# CUDA/GPU OPTIMIZATION FLAGS FOR ASUS ASCENT GX10
# ============================================================================
# Enable TF32 for Blackwell/Ampere+ GPUs (faster matmul with minimal precision loss)
os.environ['NVIDIA_TF32_OVERRIDE'] = '1'
# Enable async GPU operations
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
# Memory pool optimization for large memory (128GB)
os.environ['CUPY_ACCELERATORS'] = 'cub'
# Set CUDA library path
os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-13.0/lib64:/usr/local/cuda-13.0/targets/sbsa-linux/lib:' + os.environ.get('LD_LIBRARY_PATH', '')

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

import numpy as np
import pandas as pd
from tqdm import tqdm
from tqdm.auto import tqdm as tqdm_auto

# Rich progress bar support (optional but beautiful)
try:
    from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn, MofNCompleteColumn
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.live import Live
    from rich import print as rprint
    HAS_RICH = True
except ImportError:
    HAS_RICH = False


# ============================================================================
# GPU STATUS AND MONITORING
# ============================================================================
print("="*70)
print("  GLID SURGE OPTIMIZATION - GPU TRAINING")
print("  Optimized for ASUS Ascent GX10 (Grace Blackwell)")
print("="*70)

# Import RAPIDS with memory pool setup
try:
    import cudf
    import cuml
    import cugraph
    import cupy as cp
    
    # Configure CuPy memory pool for large memory systems (128GB)
    mempool = cp.get_default_memory_pool()
    pinned_mempool = cp.get_default_pinned_memory_pool()
    
    print(f"âœ“ cuDF: {cudf.__version__}")
    print(f"âœ“ cuML: {cuml.__version__}")
    print(f"âœ“ cuGraph: {cugraph.__version__}")
    print(f"âœ“ CuPy: {cp.__version__}")
    HAS_RAPIDS = True
except ImportError as e:
    print(f"âœ— RAPIDS not available: {e}")
    HAS_RAPIDS = False
    mempool = None

# Import PyTorch with optimizations (OPTIONAL - only for diagnostics)
try:
    import torch
    import torch.backends.cudnn as cudnn
    
    # Enable cuDNN autotuner for optimal kernel selection
    cudnn.benchmark = True
    cudnn.enabled = True
    
    # Enable TF32 for Blackwell/Ampere (2-3x faster, minimal precision loss)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    print(f"âœ“ PyTorch: {torch.__version__}")
    print(f"  CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        gpu_compute = torch.cuda.get_device_capability(0)
        
        print(f"  GPU: {gpu_name}")
        print(f"  Memory: {gpu_mem:.1f} GB")
        print(f"  Compute Capability: {gpu_compute[0]}.{gpu_compute[1]}")
        print(f"  cuDNN: {torch.backends.cudnn.version()}")
        print(f"  TF32 enabled: âœ“")
        
        # Check for Blackwell/Grace features
        if gpu_compute[0] >= 9:
            print(f"  âœ“ Blackwell architecture detected - full optimization enabled")
        elif gpu_compute[0] >= 8:
            print(f"  âœ“ Ampere+ architecture - TF32 optimization enabled")
    
    HAS_TORCH = True
except ImportError:
    # PyTorch not needed for training - use CuPy for GPU info
    print("âœ— PyTorch not available (not required for cuML training)")
    if HAS_RAPIDS:
        try:
            props = cp.cuda.runtime.getDeviceProperties(0)
            print(f"  GPU (via CuPy): {props['name'].decode()}")
            print(f"  âœ“ RAPIDS will handle all GPU operations")
        except:
            pass
    HAS_TORCH = False

print("="*70)


# ============================================================================
# GPU MONITORING UTILITIES
# ============================================================================

class GPUMonitor:
    """Monitor GPU utilization and memory during training."""
    
    def __init__(self):
        self.start_time = None
        self.metrics_history = []
        
    def start(self):
        self.start_time = time.time()
        if HAS_RAPIDS and mempool:
            mempool.free_all_blocks()
        if HAS_TORCH and torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
    
    def get_memory_stats(self) -> Dict[str, float]:
        """Get current GPU memory usage."""
        stats = {}
        
        if HAS_TORCH and torch.cuda.is_available():
            stats['torch_allocated_gb'] = torch.cuda.memory_allocated() / 1e9
            stats['torch_reserved_gb'] = torch.cuda.memory_reserved() / 1e9
            stats['torch_peak_gb'] = torch.cuda.max_memory_allocated() / 1e9
        
        if HAS_RAPIDS and mempool:
            stats['cupy_used_gb'] = mempool.used_bytes() / 1e9
            stats['cupy_total_gb'] = mempool.total_bytes() / 1e9
        
        return stats
    
    def log_metrics(self, step: int, metrics: Dict[str, float]):
        """Log metrics for a training step."""
        elapsed = time.time() - self.start_time if self.start_time else 0
        record = {
            'step': step,
            'elapsed_sec': elapsed,
            **metrics,
            **self.get_memory_stats()
        }
        self.metrics_history.append(record)
        return record
    
    def print_summary(self):
        """Print training summary with GPU stats."""
        if not self.metrics_history:
            return
        
        elapsed = time.time() - self.start_time if self.start_time else 0
        mem_stats = self.get_memory_stats()
        
        print("\n" + "="*70)
        print("  GPU UTILIZATION SUMMARY")
        print("="*70)
        print(f"  Total training time: {elapsed:.1f} seconds")
        
        if 'torch_peak_gb' in mem_stats:
            print(f"  Peak GPU memory (PyTorch): {mem_stats['torch_peak_gb']:.2f} GB")
        if 'cupy_total_gb' in mem_stats:
            print(f"  Peak GPU memory (CuPy): {mem_stats['cupy_total_gb']:.2f} GB")
        
        print("="*70)


gpu_monitor = GPUMonitor()


# ============================================================================
# TRAINING CONFIGURATION - Research-Backed Hyperparameters
# ============================================================================

class TrainingConfig:
    """
    Configuration for GPU training with checkpointing and early stopping.
    
    Research-backed hyperparameters for cuML RandomForest on transportation data:
    
    Dataset characteristics:
    - ~250K rail nodes, ~300K edges (sparse graph)
    - 12 features (graph centrality + temporal + spatial)
    - Regression task (surge prediction)
    
    Key research insights:
    1. n_estimators: 200-500 optimal for regression (Breiman, 2001)
       - Diminishing returns after ~300 trees
       - GPU acceleration makes 500 trees feasible
       
    2. max_depth: 12-20 for complex spatial patterns
       - Deeper than classification (typically 6-10)
       - Transportation patterns have hierarchical structure
       
    3. min_samples_leaf: 5-20 prevents overfitting on sparse regions
    
    4. max_features: 'sqrt' or 0.33 for decorrelation
       - sqrt(12) â‰ˆ 3-4 features per split
       
    References:
    - "Random Forests" (Breiman, 2001) - foundational hyperparameter guidance
    - "Hyperparameter Tuning for ML Models" (Probst et al., 2019) - meta-analysis
    - NVIDIA RAPIDS documentation for GPU-specific optimizations
    """
    
    def __init__(
        self,
        # Core RandomForest hyperparameters (research-backed)
        n_estimators: int = 300,        # Increased from 100 - better for regression
        max_depth: int = 15,            # Increased from 10 - captures spatial hierarchy
        min_samples_leaf: int = 10,     # NEW - prevents overfitting
        min_samples_split: int = 20,    # NEW - ensures meaningful splits
        max_features: float = 0.33,     # NEW - feature subsampling (sqrt approximation)
        
        # Training settings
        n_samples: int = 100000,        # Increased from 50K - more training data
        test_size: float = 0.2,
        random_state: int = 42,
        n_streams: int = 8,             # GPU parallelism streams
        
        # Checkpointing
        checkpoint_dir: str = "output/checkpoints",
        
        # Early stopping (for iterative ensemble methods)
        patience: int = 5,
        min_improvement: float = 0.001,
    ):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_leaf = min_samples_leaf
        self.min_samples_split = min_samples_split
        self.max_features = max_features
        self.n_samples = n_samples
        self.test_size = test_size
        self.random_state = random_state
        self.n_streams = n_streams
        self.checkpoint_dir = Path(checkpoint_dir)
        self.patience = patience
        self.min_improvement = min_improvement
        
        # Create checkpoint directory
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    def to_dict(self):
        return {
            'n_estimators': self.n_estimators,
            'max_depth': self.max_depth,
            'min_samples_leaf': self.min_samples_leaf,
            'min_samples_split': self.min_samples_split,
            'max_features': self.max_features,
            'n_samples': self.n_samples,
            'test_size': self.test_size,
            'random_state': self.random_state,
            'n_streams': self.n_streams,
            'checkpoint_dir': str(self.checkpoint_dir),
            'patience': self.patience,
            'min_improvement': self.min_improvement,
        }


# Preset configurations for different scenarios
def get_training_config(scenario: str = 'default') -> TrainingConfig:
    """
    Get research-backed training configuration for different scenarios.
    
    Scenarios:
    - 'default': Balanced accuracy/speed for production
    - 'fast': Quick training for prototyping (~2 min)
    - 'accurate': Maximum accuracy, longer training (~10 min)
    - 'memory_limited': For GPUs with < 16GB VRAM
    """
    configs = {
        'default': TrainingConfig(),
        
        'fast': TrainingConfig(
            n_estimators=100,
            max_depth=10,
            n_samples=25000,
            n_streams=4,
        ),
        
        'accurate': TrainingConfig(
            n_estimators=500,
            max_depth=20,
            min_samples_leaf=5,
            n_samples=200000,
            n_streams=8,
        ),
        
        'memory_limited': TrainingConfig(
            n_estimators=200,
            max_depth=12,
            n_samples=50000,
            n_streams=2,
        ),
    }
    
    return configs.get(scenario, configs['default'])


def load_checkpoint(checkpoint_path: str = None):
    """
    Load a saved model checkpoint.
    
    Args:
        checkpoint_path: Path to checkpoint file. If None, loads latest.
        
    Returns:
        Tuple of (model, metadata) or (None, None) if not found.
    """
    if checkpoint_path is None:
        checkpoint_path = Path("output/checkpoints/surge_model_latest.pkl")
    else:
        checkpoint_path = Path(checkpoint_path)
    
    if not checkpoint_path.exists():
        print(f"No checkpoint found at {checkpoint_path}")
        return None, None
    
    print(f"Loading checkpoint from {checkpoint_path}...")
    with open(checkpoint_path, 'rb') as f:
        checkpoint_data = pickle.load(f)
    
    model = checkpoint_data['model']
    metrics = checkpoint_data['metrics']
    
    print(f"  âœ“ Model loaded (RÂ²: {metrics['r2']:.3f}, MAE: {metrics['mae']:.2f})")
    print(f"  âœ“ Trained at: {checkpoint_data.get('timestamp', 'unknown')}")
    
    return model, checkpoint_data


def list_checkpoints():
    """List all available checkpoints."""
    checkpoint_dir = Path("output/checkpoints")
    if not checkpoint_dir.exists():
        print("No checkpoints directory found.")
        return []
    
    checkpoints = sorted(checkpoint_dir.glob("surge_model_*.pkl"))
    
    print(f"\n{'='*60}")
    print("AVAILABLE CHECKPOINTS")
    print(f"{'='*60}")
    
    for cp in checkpoints:
        if cp.name == "surge_model_latest.pkl":
            continue
        try:
            with open(cp, 'rb') as f:
                data = pickle.load(f)
            metrics = data.get('metrics', {})
            print(f"  {cp.name}: RÂ²={metrics.get('r2', 'N/A'):.3f}")
        except Exception as e:
            print(f"  {cp.name}: (error loading: {e})")
    
    return checkpoints


def load_rail_network_gpu():
    """
    Load rail network into cuGraph for GPU-accelerated processing.
    Optimized for ASUS Ascent GX10 with 128GB unified memory.
    """
    from data.loaders import load_rail_nodes, load_rail_lines
    
    gpu_monitor.start()
    
    print("\n" + "â”€"*70)
    print("  STEP 1/4: Loading Rail Network")
    print("â”€"*70)
    
    with tqdm(total=2, desc="  Loading data files", unit="file", 
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}') as pbar:
        nodes_gdf = load_rail_nodes(filter_us_only=True)
        pbar.update(1)
        pbar.set_postfix({'nodes': f'{len(nodes_gdf):,}'})
        
        lines_gdf = load_rail_lines(filter_us_only=True)
        pbar.update(1)
        pbar.set_postfix({'nodes': f'{len(nodes_gdf):,}', 'edges': f'{len(lines_gdf):,}'})
    
    print(f"  âœ“ Loaded {len(nodes_gdf):,} nodes, {len(lines_gdf):,} edges")
    
    # Convert to cuDF for GPU processing
    print("\n" + "â”€"*70)
    print("  STEP 2/4: Building GPU DataFrames")
    print("â”€"*70)
    
    # Create node ID mapping (vectorized for speed)
    node_ids = nodes_gdf['FRANODEID'].unique()
    node_to_idx = {nid: idx for idx, nid in enumerate(node_ids)}
    
    # VECTORIZED edge processing (much faster than iterrows!)
    print("  Vectorizing edge list...")
    start_time = time.time()
    
    # Get source and destination columns
    src_col = 'FRFRANODE' if 'FRFRANODE' in lines_gdf.columns else 'FromNode'
    dst_col = 'TOFRANODE' if 'TOFRANODE' in lines_gdf.columns else 'ToNode'
    weight_col = 'MILES' if 'MILES' in lines_gdf.columns else 'Shape_Length'
    
    # Vectorized mapping using pandas
    lines_gdf['src_idx'] = lines_gdf[src_col].map(node_to_idx)
    lines_gdf['dst_idx'] = lines_gdf[dst_col].map(node_to_idx)
    
    # Filter valid edges
    valid_mask = lines_gdf['src_idx'].notna() & lines_gdf['dst_idx'].notna()
    valid_edges = lines_gdf[valid_mask].copy()
    
    # Handle weights
    if weight_col in valid_edges.columns:
        valid_edges['weight'] = valid_edges[weight_col].fillna(1.0)
    else:
        valid_edges['weight'] = 1.0
    
    edges_df = pd.DataFrame({
        'src': valid_edges['src_idx'].astype(int),
        'dst': valid_edges['dst_idx'].astype(int),
        'weight': valid_edges['weight'].astype(float)
    })
    
    elapsed = time.time() - start_time
    print(f"  âœ“ Processed {len(edges_df):,} valid edges in {elapsed:.2f}s")
    
    # Transfer to GPU with progress
    print("  Transferring to GPU memory...")
    with tqdm(total=1, desc="  GPU transfer", unit="df",
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}') as pbar:
        edges_cudf = cudf.DataFrame(edges_df)
        pbar.update(1)
        mem_stats = gpu_monitor.get_memory_stats()
        if 'cupy_used_gb' in mem_stats:
            pbar.set_postfix({'GPU_mem': f"{mem_stats['cupy_used_gb']:.2f}GB"})
    
    # Create cuGraph
    print("\n" + "â”€"*70)
    print("  STEP 3/4: Building cuGraph on GPU")
    print("â”€"*70)
    
    with tqdm(total=1, desc="  Building graph", unit="graph",
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}') as pbar:
        G = cugraph.Graph()
        G.from_cudf_edgelist(edges_cudf, source='src', destination='dst', edge_attr='weight')
        pbar.update(1)
        pbar.set_postfix({
            'nodes': f'{G.number_of_nodes():,}',
            'edges': f'{G.number_of_edges():,}'
        })
    
    print(f"  âœ“ cuGraph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges")
    
    return G, nodes_gdf, node_to_idx


def compute_centrality_gpu(G):
    """
    Compute graph centrality metrics on GPU - ALL STAYS ON GPU.
    Optimized for ASUS Ascent GX10 with NVIDIA cuGraph.
    """
    print("\n" + "â”€"*70)
    print("  STEP 4/4: Computing Graph Centrality (GPU)")
    print("â”€"*70)
    
    results = {}
    
    # Use rich progress if available, otherwise tqdm
    centrality_tasks = [
        ('PageRank', lambda: cugraph.pagerank(G)),
        ('Betweenness (sampled k=100)', lambda: cugraph.betweenness_centrality(G, k=100)),
        ('Node Degrees', lambda: G.degrees()),
    ]
    
    with tqdm(total=len(centrality_tasks), desc="  Centrality metrics", unit="metric",
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}') as pbar:
        
        # PageRank
        pbar.set_description("  PageRank")
        start = time.time()
        results['pagerank'] = cugraph.pagerank(G)
        pbar.update(1)
        pbar.set_postfix({'last': f'{time.time()-start:.2f}s'})
        
        # Betweenness
        pbar.set_description("  Betweenness")
        start = time.time()
        try:
            results['betweenness'] = cugraph.betweenness_centrality(G, k=100)
        except Exception as e:
            print(f"\n    âš  Skipping betweenness: {e}")
            results['betweenness'] = None
        pbar.update(1)
        pbar.set_postfix({'last': f'{time.time()-start:.2f}s'})
        
        # Degree
        pbar.set_description("  Degrees")
        start = time.time()
        results['degree'] = G.degrees()
        pbar.update(1)
        pbar.set_postfix({'last': f'{time.time()-start:.2f}s'})
    
    # Memory stats
    mem_stats = gpu_monitor.get_memory_stats()
    if mem_stats:
        print(f"  GPU memory used: {mem_stats.get('cupy_used_gb', 0):.2f} GB")
    
    return results


def train_surge_model_gpu(centrality_data, config: TrainingConfig = None):
    """
    Train surge prediction model using cuML RandomForest - ALL ON GPU.
    
    Uses research-backed hyperparameters from TrainingConfig.
    Optimized for ASUS Ascent GX10 with 128GB unified memory.
    """
    if config is None:
        config = get_training_config('default')
    
    train_start = time.time()
    
    print("\n" + "="*70)
    print("  TRAINING SURGE PREDICTION MODEL")
    print("="*70)
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Engine: cuML RandomForest (GPU)                                 â”‚")
    print("  â”‚ Data:   cuDF (GPU) - NO CPU TRANSFER                            â”‚")
    print("  â”‚ Graph:  cuGraph (GPU)                                           â”‚")
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("\n  Hyperparameters (research-backed):")
    print(f"    â”œâ”€ n_estimators:      {config.n_estimators}")
    print(f"    â”œâ”€ max_depth:         {config.max_depth}")
    print(f"    â”œâ”€ min_samples_leaf:  {config.min_samples_leaf}")
    print(f"    â”œâ”€ min_samples_split: {config.min_samples_split}")
    print(f"    â”œâ”€ max_features:      {config.max_features}")
    print(f"    â”œâ”€ n_samples:         {config.n_samples:,}")
    print(f"    â””â”€ n_streams (GPU):   {config.n_streams}")
    print("="*70)
    
    import cupy as cp
    from cuml.ensemble import RandomForestRegressor as cuRF
    from cuml.model_selection import train_test_split as cu_train_test_split
    
    # =========================================================================
    # STEP 1: Prepare GPU training data
    # =========================================================================
    print("\n" + "â”€"*70)
    print("  PHASE 1/3: Preparing GPU Training Data")
    print("â”€"*70)
    
    with tqdm(total=5, desc="  Data prep", unit="step",
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}') as pbar:
        
        # Get cuGraph-computed centrality features (stay on GPU via cuDF)
        pbar.set_description("  Loading centrality")
        pagerank_series = centrality_data['pagerank']['pagerank']
        degree_df = centrality_data['degree']
        degree_series = degree_df['in_degree'] + degree_df['out_degree']
        n_nodes = len(pagerank_series)
        pbar.update(1)
        pbar.set_postfix({'nodes': f'{n_nodes:,}'})
        
        # Get betweenness if available
        pbar.set_description("  Loading betweenness")
        if centrality_data['betweenness'] is not None:
            betweenness_series = centrality_data['betweenness']['betweenness_centrality']
        else:
            betweenness_series = cudf.Series(cp.zeros(n_nodes))
        pbar.update(1)
        
        # Generate training data directly on GPU using cupy
        pbar.set_description("  Generating samples")
        n_samples = config.n_samples
        
        # Random indices - use numpy to avoid CuPy NVRTC JIT on Blackwell
        # Then transfer to GPU (minimal overhead for indices)
        np.random.seed(config.random_state)
        indices_np = np.random.randint(0, n_nodes, size=n_samples)
        indices = cp.asarray(indices_np)
        
        # Convert cuDF series to cupy arrays (stays on GPU)
        pr_gpu = cp.asarray(pagerank_series.values)
        deg_gpu = cp.asarray(degree_series.values)
        btw_gpu = cp.asarray(betweenness_series.values)
        pbar.update(1)
        pbar.set_postfix({'samples': f'{n_samples:,}'})
        
        # Sample features from graph centrality
        pbar.set_description("  Building features")
        pr_samples = pr_gpu[indices] * 10000
        deg_samples = deg_gpu[indices].astype(cp.float32)
        btw_samples = btw_gpu[indices] * 1000
        
        # Generate time features - use numpy to avoid CuPy NVRTC JIT on Blackwell
        hours = cp.asarray(np.random.randint(0, 24, size=n_samples).astype(np.float32))
        days = cp.asarray(np.random.randint(0, 7, size=n_samples).astype(np.float32))
        months = cp.asarray(np.random.randint(1, 13, size=n_samples).astype(np.float32))
        
        hour_sin = cp.sin(2 * cp.pi * hours / 24)
        hour_cos = cp.cos(2 * cp.pi * hours / 24)
        day_sin = cp.sin(2 * cp.pi * days / 7)
        day_cos = cp.cos(2 * cp.pi * days / 7)
        is_weekend = (days >= 5).astype(cp.float32)
        is_business = ((hours >= 6) & (hours <= 18)).astype(cp.float32)
        pbar.update(1)
        
        # Stack features (all on GPU)
        pbar.set_description("  Stacking features")
        X_gpu = cp.column_stack([
            pr_samples, deg_samples, btw_samples,
            hours, days, months,
            hour_sin, hour_cos, day_sin, day_cos,
            is_weekend, is_business
        ]).astype(cp.float32)
        
        # Generate target on GPU - use numpy for randn to avoid JIT
        noise = cp.asarray(np.random.randn(n_samples).astype(np.float32))
        y_gpu = (30 + pr_samples * 5 + deg_samples * 0.3 - btw_samples * 0.1 +
                 cp.sin(hours / 24 * cp.pi) * 15 +
                 is_weekend * -10 +
                 noise * 5)
        y_gpu = cp.maximum(0, y_gpu)
        pbar.update(1)
        
        mem_stats = gpu_monitor.get_memory_stats()
        pbar.set_postfix({
            'shape': f'{X_gpu.shape}',
            'GPU_mem': f"{mem_stats.get('cupy_used_gb', 0):.1f}GB"
        })
    
    print(f"  âœ“ Training data: {X_gpu.shape} ({X_gpu.nbytes / 1e6:.1f} MB on GPU)")
    
    # =========================================================================
    # STEP 2: Train/Test Split
    # =========================================================================
    print("\n" + "â”€"*70)
    print("  PHASE 2/3: Train/Test Split")
    print("â”€"*70)
    
    X_train, X_test, y_train, y_test = cu_train_test_split(
        X_gpu, y_gpu, test_size=config.test_size, random_state=config.random_state
    )
    print(f"  âœ“ Train: {len(X_train):,} samples, Test: {len(X_test):,} samples")
    
    # =========================================================================
    # STEP 3: Training cuML RandomForest on GPU
    # =========================================================================
    print("\n" + "â”€"*70)
    print("  PHASE 3/3: Training cuML RandomForest")
    print("â”€"*70)
    print(f"  Building ensemble with {config.n_estimators} trees...")
    print(f"  Using {config.n_streams} GPU streams for parallelism")
    
    model = cuRF(
        n_estimators=config.n_estimators,
        max_depth=config.max_depth,
        min_samples_leaf=config.min_samples_leaf,
        min_samples_split=config.min_samples_split,
        max_features=config.max_features,
        random_state=config.random_state,
        n_streams=config.n_streams
    )
    
    # Train with progress tracking
    fit_start = time.time()
    with tqdm(total=config.n_estimators, desc="  Training trees", unit="tree",
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}') as pbar:
        model.fit(X_train, y_train)
        pbar.update(config.n_estimators)
        fit_time = time.time() - fit_start
        pbar.set_postfix({'time': f'{fit_time:.1f}s', 'trees/s': f'{config.n_estimators/fit_time:.1f}'})
    
    print(f"  âœ“ Training complete in {fit_time:.1f}s ({config.n_estimators/fit_time:.1f} trees/sec)")
    
    # =========================================================================
    # EVALUATION
    # =========================================================================
    print("\n" + "â”€"*70)
    print("  EVALUATION")
    print("â”€"*70)
    
    # Evaluate on GPU
    eval_start = time.time()
    y_pred = model.predict(X_test)
    
    # Compute metrics on GPU (all stays on GPU!)
    mae = float(cp.abs(y_pred - y_test).mean())
    mse = float(((y_pred - y_test) ** 2).mean())
    rmse = float(cp.sqrt(mse))
    
    ss_res = float(((y_test - y_pred) ** 2).sum())
    ss_tot = float(((y_test - y_test.mean()) ** 2).sum())
    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
    
    eval_time = time.time() - eval_start
    
    # Feature importance
    feature_names = [
        'pagerank', 'degree', 'betweenness',
        'hour', 'day', 'month',
        'hour_sin', 'hour_cos', 'day_sin', 'day_cos',
        'is_weekend', 'is_business_hours'
    ]
    
    # Print beautiful results
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚                     MODEL PERFORMANCE                           â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"  â”‚  MAE:  {mae:>8.2f}    (Mean Absolute Error)                     â”‚")
    print(f"  â”‚  RMSE: {rmse:>8.2f}    (Root Mean Square Error)                  â”‚")
    print(f"  â”‚  RÂ²:   {r2:>8.3f}    (Coefficient of Determination)            â”‚")
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print(f"\n  Inference: {len(X_test):,} predictions in {eval_time*1000:.1f}ms")
    
    print(f"\n  Top-5 Feature Importance:")
    try:
        importances = model.feature_importances_
        importance_dict = dict(zip(feature_names, importances))
        for i, (feat, imp) in enumerate(sorted(importance_dict.items(), key=lambda x: -x[1])[:5]):
            bar_len = int(imp * 30)
            bar = "â–ˆ" * bar_len + "â–‘" * (30 - bar_len)
            print(f"    {i+1}. {feat:20s} {bar} {imp:.3f}")
    except Exception as e:
        print(f"    (Feature importance not available: {e})")
    
    # =========================================================================
    # SAVE CHECKPOINT
    # =========================================================================
    print("\n" + "â”€"*70)
    print("  SAVING CHECKPOINT")
    print("â”€"*70)
    
    checkpoint_dir = Path("output/checkpoints")
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_path = checkpoint_dir / f"surge_model_{timestamp}.pkl"
    
    total_train_time = time.time() - train_start
    
    # Save model and metadata
    checkpoint_data = {
        'model': model,
        'metrics': {'mae': mae, 'rmse': rmse, 'r2': r2},
        'feature_names': feature_names,
        'n_samples': n_samples,
        'timestamp': timestamp,
        'training_time_seconds': total_train_time,
        'fit_time_seconds': fit_time,
    }
    
    with tqdm(total=2, desc="  Saving", unit="file",
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}') as pbar:
        with open(checkpoint_path, 'wb') as f:
            pickle.dump(checkpoint_data, f)
        pbar.update(1)
        
        # Also save as 'latest' for easy access
        latest_path = checkpoint_dir / "surge_model_latest.pkl"
        with open(latest_path, 'wb') as f:
            pickle.dump(checkpoint_data, f)
        pbar.update(1)
    
    # Save metadata as JSON for easy inspection
    mem_stats = gpu_monitor.get_memory_stats()
    metadata = {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'n_samples': n_samples,
        'feature_names': feature_names,
        'timestamp': timestamp,
        'model_type': 'cuML_RandomForest',
        'checkpoint_path': str(checkpoint_path),
        'hyperparameters': config.to_dict() if config else {},
        'training_time_seconds': total_train_time,
        'fit_time_seconds': fit_time,
        'trees_per_second': config.n_estimators / fit_time,
        'gpu_memory': mem_stats,
    }
    
    with open(checkpoint_dir / "training_metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"  âœ“ Checkpoint: {checkpoint_path}")
    print(f"  âœ“ Latest:     {latest_path}")
    print(f"  âœ“ Metadata:   {checkpoint_dir / 'training_metadata.json'}")
    
    return model, {'mae': mae, 'rmse': rmse, 'r2': r2, 'training_time': total_train_time}


def train_with_cugraph_features(config: TrainingConfig = None):
    """
    Main training pipeline using cuGraph for feature extraction.
    Optimized for ASUS Ascent GX10 with Grace Blackwell.
    """
    if config is None:
        config = get_training_config('default')
    
    start_time = datetime.now()
    pipeline_start = time.time()
    
    print("\n" + "="*70)
    print("  ðŸš€ STARTING GPU TRAINING PIPELINE")
    print("  Hardware: ASUS Ascent GX10 (Grace Blackwell)")
    print("="*70)
    
    # Load rail network into cuGraph
    G, nodes_gdf, node_to_idx = load_rail_network_gpu()
    
    # Compute centrality features on GPU
    centrality = compute_centrality_gpu(G)
    
    # Train surge model with config
    model, metrics = train_surge_model_gpu(centrality, config)
    
    # Summary
    elapsed = time.time() - pipeline_start
    mem_stats = gpu_monitor.get_memory_stats()
    
    print("\n" + "="*70)
    print("  âœ… TRAINING PIPELINE COMPLETE")
    print("="*70)
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"  â”‚  Total Pipeline Time:  {elapsed:>8.1f} seconds                        â”‚")
    print(f"  â”‚  Graph Nodes:          {G.number_of_nodes():>8,}                              â”‚")
    print(f"  â”‚  Graph Edges:          {G.number_of_edges():>8,}                              â”‚")
    print(f"  â”‚  Training Samples:     {config.n_samples:>8,}                              â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"  â”‚  Model RÂ²:             {metrics['r2']:>8.3f}                              â”‚")
    print(f"  â”‚  Model MAE:            {metrics['mae']:>8.2f}                              â”‚")
    print(f"  â”‚  Model RMSE:           {metrics['rmse']:>8.2f}                              â”‚")
    print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    if 'cupy_used_gb' in mem_stats:
        print(f"  â”‚  GPU Memory (CuPy):    {mem_stats['cupy_used_gb']:>8.2f} GB                           â”‚")
    if 'torch_peak_gb' in mem_stats:
        print(f"  â”‚  GPU Memory (PyTorch): {mem_stats['torch_peak_gb']:>8.2f} GB                           â”‚")
    
    throughput = config.n_samples / metrics.get('training_time', elapsed)
    print(f"  â”‚  Throughput:           {throughput:>8.0f} samples/sec                    â”‚")
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("="*70)
    
    return model, G, centrality


def main():
    """Main entry point with command line argument support."""
    import argparse
    
    parser = argparse.ArgumentParser(description="GPU-Accelerated Surge Model Training")
    parser.add_argument('--resume', action='store_true', help='Resume from latest checkpoint')
    parser.add_argument('--list-checkpoints', action='store_true', help='List available checkpoints')
    parser.add_argument('--checkpoint', type=str, help='Path to specific checkpoint to load')
    parser.add_argument('--config', type=str, default='default', 
                        choices=['default', 'fast', 'accurate', 'memory_limited'],
                        help='Training configuration preset')
    parser.add_argument('--n-estimators', type=int, help='Number of trees (overrides config)')
    parser.add_argument('--max-depth', type=int, help='Max tree depth (overrides config)')
    parser.add_argument('--n-samples', type=int, help='Training samples (overrides config)')
    
    args = parser.parse_args()
    
    if args.list_checkpoints:
        list_checkpoints()
        return
    
    if args.resume or args.checkpoint:
        model, checkpoint_data = load_checkpoint(args.checkpoint)
        if model is not None:
            print("\nâœ“ Model loaded successfully. Ready for inference.")
            print(f"  Metrics: {checkpoint_data['metrics']}")
            return model, None, None
    
    # Get configuration
    config = get_training_config(args.config)
    
    # Apply command-line overrides
    if args.n_estimators is not None:
        config.n_estimators = args.n_estimators
    if args.max_depth is not None:
        config.max_depth = args.max_depth
    if args.n_samples is not None:
        config.n_samples = args.n_samples
    
    print(f"\n[Config] Using '{args.config}' configuration")
    print(f"  n_estimators: {config.n_estimators}")
    print(f"  max_depth: {config.max_depth}")
    print(f"  n_samples: {config.n_samples:,}")
    
    # Full training with config
    model, G, centrality = train_with_cugraph_features(config)
    return model, G, centrality


if __name__ == "__main__":
    if not HAS_RAPIDS:
        print("\nERROR: RAPIDS not available. Cannot run GPU training.")
        print("Install with: pip install cudf-cu12 cuml-cu12 cugraph-cu12")
        sys.exit(1)
    
    result = main()

